<html>
<head>
    <meta charset="utf-8">
    <title>主题模型及LSI、PLSI、LDA简介 - Joking</title>
    <link rel="stylesheet" href="../static/css/bootstrap.min.css">
</head>
<body>
<div class="container">
    <div class="blog-header">
        <h1 class="blog-title">Joking</h1>
        <hr/>
    </div>
    <div class="row col-sm-12 blog-main">
        <div class="blog-post">
            <h3 class="blog-post-title">
                主题模型及LSI、PLSI、LDA简介
            </h3>
            <p>如何从大量文档中自动归纳出每篇文章的主题，是一个很有意思的任务。
1998年，Papadimitriou等人在<a href="http://www.ingentaconnect.com/content/ap/ss/2000/00000061/00000002/art01711">一篇论文</a>
中提出了LSI的概念，这是一种主题模型（topic model），
<a href="http://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">维基百科上面的主题模型词条</a>。</p>
<p>在这篇文章中提出了LSI（潜在语义索引），维基百科词条
<a href="http://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">潜在语义索引</a></p>
<p>PLSI指的是概率性潜在语义索引。跟PLSA是同义语，维基百科上面这两个词也指向同一个页面。
<a href="http://cs.brown.edu/~th/papers/Hofmann-SIGIR99.pdf">论文下载</a></p>
<p>LSI从那里来的呢？从LSA而来。
<a href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent semantic analysis</a>，
<a href="http://lsa.colorado.edu/">LSA主页</a>，
<a href="http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf">论文下载</a>，
还有一个概念是PLSA，
<a href="http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">Probabilistic latent semantic analysis</a>，
它来自于<a href="http://cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf">这篇论文</a>。
PLSI和PLSA的两篇论文都是hofmann所发表。</p>
<p>2003年，Blei等人提出LDA，这是一种一般化的PLSI，之后其他的主题模型一般是在LDA的基础上改进的。
维基百科<a href="http://zh.wikipedia.org/wiki/%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E9%85%8D">隐含狄利克雷分布</a>。
论文地址：<a href="http://jmlr.org/papers/v3/blei03a.html">Latent Dirichlet Allocation</a>
现任百度公司首席科学家的<a href="http://zh.wikipedia.org/wiki/%E5%90%B4%E6%81%A9%E8%BE%BE">吴恩达</a>是论文的作者之一。</p>
<p>中文方面，丕子网有一个<a href="http://www.zhizhihu.com/html/ytag/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">关于主题模型</a>的系列文章，介绍了不少有价值的信息来源。
比如，这一篇<a href="http://www.zhizhihu.com/html/y2011/3226.html">收集下2010之前的“基于LDA的Topic Model变形”的论文</a>就介绍了不少这方面的论文。</p>
<p><a href="http://blog.sina.com.cn/s/blog_9d7bca9f01015580.html">八灵九霖的博客</a>里的一篇文章，虽然篇幅较短，却是最容易理解的一篇文章。</p>
<p><a href="http://cos.name/">统计之都</a>上面也有一些此方面的论文，搜索一下LDA等词即可搜到，比如<a href="http://cos.name/2013/03/lda-math-text-modeling/">这篇文章</a>。</p>
<p>我们要提倡创新精神、创造性思维、创造性劳动。比如，上面这许多中文博客，内容都有一定价值，但是没有原创内容，属于对大牛的原创内容的介绍与二次传播。
我们虽然要写博客记录、介绍和学习大牛的成果，自己也要做一个生产者，通过自己的实践和思考，产生高质量的原创性成果。</p>
<p>如果想要学习这个领域的最新成果，可以找一下上面几篇大牛论文所发表的期刊和会议，然后看一下这些期刊和会议的最新几期的论文，应该能够了解到该领域最新
前沿成果。</p>
        </div>
    </div>
</div>
<script src="static/js/jquery.min.js"></script>
<script src="static/js/bootstrap.min.js"></script>
</body>
</html>